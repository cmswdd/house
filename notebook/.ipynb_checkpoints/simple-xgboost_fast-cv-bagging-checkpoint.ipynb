{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 0 of 11 folds\n",
      "[0]\tvalidation_0-rmse:7.75335\n",
      "Will train until validation_0-rmse hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-rmse:4.19812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-33c45ea3168a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Full model run\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-33c45ea3168a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m    291\u001b[0m                                                     \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tradeMoney'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                                                     \u001b[0mnr_folds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                                                     verbose=100)\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-33c45ea3168a>\u001b[0m in \u001b[0;36mmodeling_lgbm_cross_validation\u001b[1;34m(params, X, y, nr_folds, verbose)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         )\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\xgboost-0.81-py3.6.egg\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    376\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\xgboost-0.81-py3.6.egg\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\xgboost-0.81-py3.6.egg\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\xgboost-0.81-py3.6.egg\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1085\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1086\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(asctime)s %(filename)s: %(lineno)d: %(message)s',\n",
    "    datefmt='%Y-%m-%d:%H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "\n",
    "DATE_TODAY = dt(2019, 1, 26)\n",
    "\n",
    "FEATS_EXCLUDED = [\n",
    "    \"ID\",\"tradeMoney\"\n",
    "    ]\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    categorical_columns = list(filter(lambda c: c in ['object'], df.dtypes))\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "\n",
    "    new_columns = list(filter(lambda c: c not in original_columns, df.columns))\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def modeling_lgbm_cross_validation(params, X, y, nr_folds=5, verbose=0):\n",
    "    clfs = list()\n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    # Split data with kfold\n",
    "    # kfolds = TimeSeriesSplit(n_splits=nr_folds)\n",
    "    kfolds = StratifiedKFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "    #split_index = X[['feature_1', 'feature_2', 'feature_3']].apply(lambda x: np.log1p(x)).product(axis=1)\n",
    "    kfolds = KFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):\n",
    "        if verbose:\n",
    "            print('no {} of {} folds'.format(n_fold, nr_folds))\n",
    "\n",
    "        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        #y_train, y_valid= np.log1p(y_train.values) , np.log1p(y_valid.values)\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=verbose, eval_metric='rmse',\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "\n",
    "        clfs.append(model)\n",
    "        oof_preds[val_idx] = model.predict(X_valid, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    \n",
    "    np.savetxt(\"xgb_oof_preds.csv\", np.expm1(oof_preds), delimiter=\",\")    \n",
    "    #score = mean_squared_error(y, oof_preds) ** .5\n",
    "\n",
    "    score = r2_score(y, oof_preds)\n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def predict_cross_validation(test, clfs, ntree_limit=None):\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    for i, model in enumerate(clfs, 1):\n",
    "\n",
    "        num_tree = 10000\n",
    "        if not ntree_limit:\n",
    "            ntree_limit = num_tree\n",
    "\n",
    "        if isinstance(model, lgb.sklearn.LGBMRegressor):\n",
    "            if model.best_iteration_:\n",
    "                num_tree = min(ntree_limit, model.best_iteration_)\n",
    "\n",
    "            test_preds = model.predict(test, raw_score=True, num_iteration=num_tree)\n",
    "\n",
    "        if isinstance(model, xgb.sklearn.XGBRegressor):\n",
    "            num_tree = min(ntree_limit, model.best_ntree_limit)\n",
    "            test_preds = model.predict(test, ntree_limit=num_tree)\n",
    "\n",
    "        sub_preds += test_preds\n",
    "\n",
    "    sub_preds = sub_preds / len(clfs)\n",
    "    sub_preds = np.expm1(sub_preds)\n",
    "    np.savetxt(\"xgb_sub_preds.csv\", sub_preds, delimiter=\",\")\n",
    "    ret = pd.Series(sub_preds, index=test.index)\n",
    "    ret.index.name = test.index.name\n",
    "    return ret\n",
    "\n",
    "\n",
    "def write_to_parquet(filename, df, debug=False):\n",
    "    print('write to {}: {}'.format(filename, df.shape))\n",
    "\n",
    "    # safety check\n",
    "    cols_type = df.dtypes.to_dict()\n",
    "    for col, col_type in cols_type.items():\n",
    "        if str(col_type).startswith('float16'):\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    df.to_parquet(filename, engine='auto', compression='snappy')\n",
    "    if debug:\n",
    "        df = pd.read_parquet(filename)\n",
    "        print('debug reload save file: {}\\n{}'.format(df.shape, df.head().T))\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    \n",
    "               \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        train_df = pd.read_csv('train_clean4.csv',encoding = 'gbk')\n",
    "        test_df = pd.read_csv('test_clean4.csv',encoding = 'gbk')\n",
    "\n",
    "        train_df = train_df[train_df.tradeMoney < 100000]\n",
    "        train_df = train_df[train_df.tradeMoney > 500]\n",
    "\n",
    "        train_df = train_df[train_df.area < 2500]\n",
    "\n",
    "        train_features = [c for c in train_df.columns if c not in FEATS_EXCLUDED]\n",
    "\n",
    "        '''train_df['area'] = np.log1p(train_df['area'])\n",
    "\n",
    "        for i in train_features:\n",
    "            if train_df[i].mean() > 2500:\n",
    "                #print(i)\n",
    "                train_df[i] = np.log1p(train_df[i])'''\n",
    "                \n",
    "       \n",
    "        best_params = {\n",
    "            'gpu_id': 0, \n",
    "            #'n_gpus': 2, \n",
    "            'objective': 'reg:linear', \n",
    "            'eval_metric': 'rmse', \n",
    "            'silent': True, \n",
    "            'booster': 'gbtree', \n",
    "            'n_jobs': 4, \n",
    "            'n_estimators': 2500, \n",
    "            #'tree_method': 'gpu_hist', \n",
    "            'grow_policy': 'lossguide', \n",
    "            'max_depth': 13, \n",
    "            'seed': 538, \n",
    "            'colsample_bylevel': 0.4577985063107066, \n",
    "            'colsample_bytree': 0.8971621421463886, \n",
    "            'gamma': 0.007219683251171169, \n",
    "            'learning_rate': 0.006150886706231842, \n",
    "            'num_leaves': 46, \n",
    "            'max_bin': 16,\n",
    "            'min_child_weight': 6.584851275015851, \n",
    "            'reg_alpha': 1.476515526719819,\n",
    "            'reg_lambda': 5.040088958844647, \n",
    "            'subsample': 0.7792358657530063}\n",
    "\n",
    "        '''best_params = {\n",
    "            'gpu_id': 0, \n",
    "            #'n_gpus': 2, \n",
    "            'objective': 'reg:linear', \n",
    "            'eval_metric': 'rmse', \n",
    "            'silent': True, \n",
    "            'booster': 'gbtree', \n",
    "            'n_jobs': 4, \n",
    "            'n_estimators': 2500, \n",
    "            #'tree_method': 'gpu_hist', \n",
    "            'grow_policy': 'lossguide', \n",
    "            'max_depth': 12, \n",
    "            'seed': 538, \n",
    "            'colsample_bylevel': 0.9, \n",
    "            'colsample_bytree': 0.8, \n",
    "            'gamma': 0.0001, \n",
    "            'learning_rate': 0.006150886706231842, \n",
    "            'max_bin': 16, \n",
    "            'max_leaves': 47, \n",
    "            'min_child_weight': 40, \n",
    "            'reg_alpha': 10.0, \n",
    "            'reg_lambda': 10.0, \n",
    "            'subsample': 0.9}'''\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # modeling\n",
    "        nr_folds = 11\n",
    "        if debug:\n",
    "            nr_folds = 2\n",
    "            \n",
    "        best_params.update({'n_estimators': 200000})\n",
    "        #best_params.update({'n_estimators': 20})\n",
    "        \n",
    "        clfs = list()\n",
    "        score = 0\n",
    "        clfs, score = modeling_lgbm_cross_validation(best_params,\n",
    "                                                    train_df[train_features],\n",
    "                                                    train_df['tradeMoney'],\n",
    "                                                    nr_folds,\n",
    "                                                    verbose=100)\n",
    "\n",
    "                \n",
    "        # save to\n",
    "        file_template = '{score:.6f}_{model_key}_cv{fold}_{timestamp}'\n",
    "        file_stem = file_template.format(\n",
    "            score=score,\n",
    "            model_key='XGB',\n",
    "            fold=nr_folds,\n",
    "            timestamp=dt.now().strftime('%Y-%m-%d-%H-%M'))\n",
    "\n",
    "        \n",
    "\n",
    "        filename = 'subm_{}.csv'.format(file_stem)\n",
    "        print('save to {}'.format(filename))\n",
    "        subm = predict_cross_validation(test_df[train_features], clfs)\n",
    "        subm = subm.to_frame('target')\n",
    "        subm.to_csv(filename, index=True)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Full model run\"):\n",
    "        main(debug=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
